<!DOCTYPE HTML>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Weizhen Fang</title>
    <meta name="author" content="Weizhen Fang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-150979299-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-150979299-1');
</script>


<body>
    <table
        style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">

                    <!-- Short bio -->
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.5%;width:63%;vertical-align:middle">
                                    <p style="text-align:center">
                                        <name>Weizhen Fang (方唯振)</name>
                                    </p>
                                    <!-- <p>I am a second year PhD researcher at <a
                                            href="https://www.bosch.com/research/">Bosch Research</a> in the Stuttgart
                                        area, Germany. My research is centered on robust perception in autonomous
                                        driving. My supervisor is
                                        <a
                                            href="https://www.uni-ulm.de/in/mrm/institut/mitarbeiter/institutsleitung/prof-dr-ing-klaus-dietmayer/">Prof.
                                            Dr. Klaus Dietmayer</a> from the <a href="https://www.uni-ulm.de/en/">Ulm
                                            University</a>.
                                    </p> -->
                                    <p>
                                        I am passionate about Earth Science and Computer Science. I received the M.S.
                                        degree in Photogrammetry and Remote Sensing from Peking University in 2020. I am
                                        currently working as an engineer in the Institute of Computing Technology,
                                        Chinese Academy of Sciences, Beijing, China.
                                    </p>
                                    <p>
                                        Prior joining Bosch, I finished my master's study with distinction at the <a
                                            href="https://www.tum.de/en/">Technical University of Munich</a>. I worked
                                        on tactile intelligence in humanoid robots, under the supervision
                                        from <a href="https://de.linkedin.com/in/mohsen-kaboli-25634569">Dr. Mohsen
                                            Kaboli</a> and <a href="https://www.ics.ei.tum.de/en/people/cheng/">Prof.
                                            Dr. Gordon Cheng</a>. I also worked as research intern at the
                                        <a
                                            href="https://www.dlr.de/rm/en/desktopdefault.aspx/tabid-8023/11888_read-28100/">
                                            Institute of
                                            Robotics and Mechatronics (German Aerospace Center)</a>, as well as in the
                                        BMW autonomous driving team. I obtained my Bachelor's degree with honor from the
                                        <a href="https://www.tongji.edu.cn/">Tongji University</a>.
                                    </p>
                                    <p style="text-align:center">
                                        <a href="weizhenfang@ict.ac.cn">Email</a> &nbsp/&nbsp
                                        <!-- <a href="data/Di_Feng_Resume.pdf">CV</a> &nbsp/&nbsp -->
                                        <a href="https://www.researchgate.net/profile/Weizhen_Fang2">ResearchGate</a>
                                        &nbsp/&nbsp
                                        <a
                                            href="https://www.linkedin.com/in/william-fang-%E6%96%B9%E5%94%AF%E6%8C%AF-897504102/">
                                            LinkedIn </a>
                                    </p>
                                </td>
                                <td style="padding:2.5%;width:25%;max-width:25%">
                                    <a href="images/bio_img.png"><img style="width:100%;max-width:100%"
                                            alt="profile photo" src="images/bio_img.png" class="hoverZoomLink"></a>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- Research interests -->
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Research</heading>
                                    <p>
                                        I'm interested in machine learning in robotics, computer vision, autonomous
                                        driving and tactile sensing. Much of my current research is about leveraging
                                        multi-modal sensors for robust, probabilistic object detection networks in
                                        autonomous driving. Keywords:
                                        mutli-modal sensors, uncertainty estimation, deep learning, object detection,
                                        autonomous driving.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- Academic services and students -->

                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td>
                                    <heading>Academic Service</heading>
                                    <p> Reviewer for multiple IEEE conferences about robotics and autonomous driving,
                                        incl. <a href="https://site.ieee.org/itss/conference/itsc/">ITSC</a>, <a
                                            href="https://site.ieee.org/itss/conference/iv/">IV</a>, <a
                                            href="https://www.ieee-ras.org/conferences-workshops/fully-sponsored/icra">ICRA</a>.
                                    </p>

                                    <p>Supervising several talented master students </p>

                                    <ul>
                                        <li><strong>Xiao Wei</strong>: master's thesis "Deep Active Learning for LiDAR
                                            3D Object Detection", from <a href="https://www.kth.se/en">KTH Royal
                                                Institute of Technology</a>. Now at <a
                                                href="https://en.xiaopeng.com/">Xpeng Motors</a>.
                                        </li>
                                        <li><strong>Fabian Duffhauss</strong>: internship with the topic deep
                                            multi-modal perception, from
                                            <a href="https://www.rwth-aachen.de/go/id/a/?lidx=1">RWTH Aachen</a>. Now at
                                            Daimler autonomous driving team. </li>
                                        <li><strong>Yifan Cao</strong>: master's thesis "Teaching the Robustness of a
                                            Multi-modal Object Detector Using Probabilistic Inference", from <a
                                                href="https://www.kit.edu/english/">KIT Karlsruhe
                                                Institute of Technology</a>. </li>
                                        <li><strong>Kumar Gaurav</strong>: incoming master's thesis, from <a
                                                href="https://www.tum.de/en/">Technical University of Munich</a>. </li>
                                    </ul>

                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- NEWS -->
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>News</heading>
                                    <p>
                                        <strong>[Oct 10, 2019]</strong> I will give an invited talk about "Uncertainty
                                        Estimation in Deep Object Detectors" in the IROS 2019 workshop <a
                                            href="https://nikosuenderhauf.github.io/roboticvisionchallenges/iros2019">The
                                            Importance of
                                            Uncertainty in Deep Learning for Robotics</a>, which will be held on Nov 8,
                                        2019 (Macau, China). Thanks <a href="https://nikosuenderhauf.github.io/">Dr.
                                            Niko Suenderhauf</a> et al. for the invitation!
                                    </p>
                                    <p></p>
                                    <p>
                                        <strong>[Apr 15, 2019]</strong> Our survey paper <a
                                            href="https://arxiv.org/pdf/1909.12358.pdf">Deep
                                            Multi-modal Object Detection and Semantic Segmentation for Autonomous
                                            Driving: Datasets, Methods,
                                            and Challenges</a> has achieved in ResearchGate as <strong>the most-read
                                            Preprint in
                                            Germany</strong> for one week and eight weeks in Bosch research! We will
                                        frequently summarize the new methods and update the paper.
                                    </p>

                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- Publications -->
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Publications</heading>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr onmouseout="font_stop()" onmouseover="font_start()">

                                <!-- T-ITS 2019 -->
                            <tr onmouseout="font_stop()" onmouseover="font_start()">
                                <td style="padding:20px;width:25%;vertical-align:middle"><img
                                        style="max-height:180px;max-width:180px" src="images/tits_2019.PNG"></td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/pdf/1909.12358.pdf">
                                        <papertitle>Recognizing Global Reservoirs From Landsat 8 Images: A Deep Learning Approach</papertitle>
                                    </a>
                                    <br>
                                    <strong>Weizhen Fang</strong>, Cunguang Wang, Lars Rosenbaum, Heinz Hertlein,
                                    Claudius Glaeser, Fabian Timm, Werner Wiesbeck, Klaus Dietmayer
                                    <br>
                                    <em>IEEE Transactions on Intelligent Transportation Systems </em>, 2019 (minor
                                    revision)
                                    <br>
                                    <br>
                                    <a href="https://multimodalperception.github.io/">Online interactive platform</a>
                                    <br>
                                    <p></p>
                                    <p>Systematically summarizing methodologies and discussing challenges for deep
                                        multi-modal object detection and semantic segmentation in autonomous driving.
                                    </p>
                                </td>
                            </tr>

                            <!-- IROS 2019 -->
                            <tr onmouseout="font_stop()" onmouseover="font_start()">
                                <td style="padding:20px;width:25%;vertical-align:middle"><img
                                        style="max-height:180px;max-width:180px" src="images/iros_2019.PNG"></td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/pdf/1909.12358.pdf">
                                        <papertitle>Can We Trust You? On Calibration of a Probabilistic Object Detector
                                            for Autonomous Driving
                                        </papertitle>
                                    </a>
                                    <br>
                                    <strong>Di Feng</strong>, Lars Rosenbaum, Claudius Glaeser, Fabian Timm, Klaus
                                    Dietmayer
                                    <br>
                                    <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
                                        Workshop </em>, 2019
                                    <br>
                                    <br>
                                    <a href="https://youtu.be/pH5qT11vmyM">Video</a>
                                    <br>
                                    <p></p>
                                    <p>Identify uncertainty miscalibration problem in a state-of-the-art detector. </p>
                                    <p>Proposing three practical methods to recalibrate uncertainties.</p>
                                </td>
                            </tr>

                            <!-- IV 2019a -->
                            <tr onmouseout="font_stop()" onmouseover="font_start()">
                                <td style="padding:20px;width:25%;vertical-align:middle"><img
                                        style="max-height:180px;max-width:180px" src="images/iv_2019a.png"></td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/abs/1809.05590">
                                        <papertitle>Leveraging Heteroscedastic Aleatoric Uncertainties for Robust
                                            Real-Time LiDAR 3D Object Detection
                                        </papertitle>
                                    </a>
                                    <br>
                                    <strong>Di Feng</strong>, Lars Rosenbaum, Fabian Timm, Klaus Dietmayer
                                    <br>
                                    <em>IEEE Intelligent Vehicles Symposium (IV) </em>, 2019 &nbsp
                                    <font color="red"><strong>(Oral
                                            Presentation)</strong></font>
                                    <br>
                                    <br>
                                    <a href=" https://youtu.be/2DzH9COLpkU">Video</a>
                                    <br>
                                    <p></p>
                                    <p>Boosting detection performance by modeling aleatoric uncertainties in an object
                                        detector.</p>
                                </td>
                            </tr>

                            <!-- IV 2019b -->
                            <tr onmouseout="font_stop()" onmouseover="font_start()">
                                <td style="padding:20px;width:25%;vertical-align:middle"><img
                                        style="max-height:180px;max-width:180px" src="images/iv_2019b.png"></td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/pdf/1901.10609.pdf">
                                        <papertitle>Deep Active Learning for Efficient Training of a LiDAR 3D Object
                                            Detector</papertitle>
                                    </a>
                                    <br>
                                    <strong>Di Feng</strong>, Xiao Wei, Lars Rosenbaum, Atsuto Maki, Klaus Dietmayer
                                    <br>
                                    <em>IEEE Intelligent Vehicles Symposium (IV) </em>, 2019
                                    <br>
                                    <p></p>
                                    <p>Increasing training efficiency of a LiDAR detector by uncertainty estimation and
                                        active learning.
                                    </p>
                                </td>
                            </tr>

                            <!-- ITSC 2018 -->
                            <tr onmouseout="font_stop()" onmouseover="font_start()">
                                <td style="padding:20px;width:25%;vertical-align:middle"><img
                                        style="max-height:180px;max-width:180px" src="images/itsc_2018.PNG"></td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/pdf/1804.05132.pdf">
                                        <papertitle>Towards Safe Autonomous Driving: Capture Uncertainty in the Deep
                                            Neural Network For Lidar 3D Vehicle Detection</papertitle>
                                    </a>
                                    <br>
                                    <strong>Di Feng</strong>, Lars Rosenbaum, Klaus Dietmayer
                                    <br>
                                    <em>IEEE International Conference on Intelligent Transportation Systems (ITSC)
                                    </em>, 2018
                                    <br>
                                    <br>
                                    <a href="https://youtu.be/bQJmssB80oM">Video</a>
                                    <br>
                                    <p></p>
                                    <p>Modeling epistemic & aleatoric uncertainties in a LiDAR 3D object detector.</p>
                                    <p>Showing that both uncertianties incorporate very different information.</p>
                                </td>
                            </tr>

                            <!-- Autonomous Robot 2019 -->
                            <tr onmouseout="font_stop()" onmouseover="font_start()">
                                <td style="padding:20px;width:25%;vertical-align:middle"><img
                                        style="max-height:180px;max-width:180px" src="images/ar_2019.PNG"></td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://link.springer.com/article/10.1007/s10514-018-9707-8">
                                        <papertitle>Tactile-based active object discrimination and target object search
                                            in an unknown workspace
                                        </papertitle>
                                    </a>
                                    <br> Mohsen Kaboli, Kunpeng Yao,
                                    <strong>Di Feng</strong>, Gordon Cheng
                                    <br>
                                    <em>Autonomous Robots</em>, 2019
                                    <br>
                                    <br>
                                    <a href="https://link.springer.com/article/10.1007/s10514-018-9707-8">Video</a>
                                    <br>
                                    <p></p>
                                    <p>An autonomous robot explores unknown workspaces and recognizes objects purely
                                        based on the tactile information.
                                    </p>
                                </td>
                            </tr>

                            <!-- Sensors 2018 -->
                            <tr onmouseout="font_stop()" onmouseover="font_start()">
                                <td style="padding:20px;width:25%;vertical-align:middle"><img
                                        style="max-height:180px;max-width:180px" src="images/sensors_2018.PNG"></td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://www.mdpi.com/1424-8220/18/2/634">
                                        <papertitle>Active Prior Tactile Knowledge Transfer for Learning Tactual
                                            Properties of New Objects
                                        </papertitle>
                                    </a>
                                    <br>
                                    <strong>Di Feng</strong>, Mohsen Kaboli, Gordon Cheng
                                    <br>
                                    <em>MDPI Sensors </em>, 2018
                                    <br>
                                    <p></p>
                                    <p>Enabling a robotic arm to actively transfer prior tactile knowledge, when it
                                        learns the physical properties of new objects via multi-modal artificial skin.
                                    </p>
                                </td>
                            </tr>

                            <!-- IJHR 2017 -->
                            <tr onmouseout="font_stop()" onmouseover="font_start()">
                                <td style="padding:20px;width:25%;vertical-align:middle"><img
                                        style="max-height:180px;max-width:180px" src="images/ijhr_2017.PNG"></td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://www.worldscientific.com/doi/abs/10.1142/S0219843618500019">
                                        <papertitle>Active Tactile Transfer Learning for Object Discrimination in an
                                            Unstructured Environment using Multimodal Robotic Skin</papertitle>
                                    </a>
                                    <br> Mohsen Kaboli,
                                    <strong>Di Feng</strong>, Gordon Cheng
                                    <br>
                                    <em>International Journal of Humanoid Robotics </em>, 2017
                                    <br>
                                    <br>
                                    <a href="http://web.ics.ei.tum.de/~mohsen/videos/IJHR2017.mp4">Video</a>
                                    <br>
                                    <p></p>
                                    <p>The robot actively learns the physical properties of new objects with only a few
                                        exploratory actions or even one.</p>
                                </td>
                            </tr>

                        </tbody>
                    </table>




                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:0px">
                                    <p style="text-align:right;font-size:small;">
                                        Website by the courtesy of <a href="https://jonbarron.info/">Jon Barron</a>.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                </td>
            </tr>
    </table>
</body>

</html>